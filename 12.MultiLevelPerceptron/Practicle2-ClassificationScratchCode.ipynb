{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "023c7f82-f095-492e-b6da-7fb79cd00051",
   "metadata": {},
   "source": [
    "**`__init__` method:** \n",
    "This is the constructor method that initializes the neural network object. It takes a parameter `layer_dims`, which is a list containing the dimensions of each layer in the neural network. Inside the constructor, the `initialize_parameters` method is called to initialize the weights and biases of the neural network.\n",
    "\n",
    "**`initialize_parameters` method:** \n",
    "This method initializes the parameters (weights and biases) of the neural network with random values. It takes `layer_dims` as input, which specifies the number of nodes in each layer of the network. It initializes the weight matrices (`W`) with random values and sets the bias vectors (`b`) to zeros.\n",
    "\n",
    "**`sigmoid` method:** \n",
    "This method implements the sigmoid activation function, which is used to introduce non-linearity into the neural network. It takes the input `Z` and computes the sigmoid activation `A`.\n",
    "\n",
    "**`linear_forward` method:** \n",
    "This method performs the linear transformation step for a single layer in the neural network. It takes the previous layer's activations (`A_prev`), the weight matrix (`W`), and the bias vector (`b`) as inputs. It computes the linear combination `Z = W.T * A_prev + b` and applies the sigmoid activation function to compute the output activations `A`.\n",
    "\n",
    "**`L_layer_forward` method:** \n",
    "This method implements the forward propagation step for the entire neural network. It iterates through each layer of the network, applies the linear transformation followed by the sigmoid activation function, and computes the output activations `A`. It returns the final output activations `A` and the activations of the previous layer `A_prev`.\n",
    "\n",
    "**`update_parameters` method:** \n",
    "This method updates the parameters (weights and biases). It iterates through multiple epochs, where each epoch consists of iterating through each training example, computing the predicted output, updating the parameters, and calculating the loss. It prints the loss after each epoch. Finally, it returns the trained parameters of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35165ce9-d0f7-42a5-8679-62968b0257a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetworkClassification:\n",
    "    def __init__(self, layer_dimensions):\n",
    "        self.parameters = self.initialize_parameters(layer_dimensions)\n",
    "    \n",
    "    def initialize_parameters(self, layer_dimensions):\n",
    "        np.random.seed(3)\n",
    "        parameters = {}\n",
    "        num_layers = len(layer_dimensions)\n",
    "\n",
    "        for l in range(1, num_layers):\n",
    "            input_dim = layer_dimensions[l-1]\n",
    "            output_dim = layer_dimensions[l]\n",
    "            parameters['W' + str(l)] = np.ones((input_dim, output_dim)) * 0.1\n",
    "            parameters['b' + str(l)] = np.zeros((output_dim, 1))\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "        return A\n",
    "\n",
    "    def linear_forward(self, previous_activations, weights, biases):\n",
    "        Z = np.dot(weights.T, previous_activations) + biases\n",
    "        A = self.sigmoid(Z)\n",
    "        return A\n",
    "\n",
    "    def L_layer_forward(self, X):\n",
    "        activations = X\n",
    "        num_layers = len(self.parameters) // 2\n",
    "\n",
    "        for l in range(1, num_layers + 1):\n",
    "            previous_activations = activations\n",
    "            weights = self.parameters['W' + str(l)]\n",
    "            biases = self.parameters['b' + str(l)]\n",
    "            activations = self.linear_forward(previous_activations, weights, biases)\n",
    "\n",
    "        return activations, previous_activations\n",
    "\n",
    "    def update_parameters(self, target_value, predicted_value, previous_activations, X, learning_rate=0.001):\n",
    "        for l in range(1, len(self.parameters) // 2 + 1):\n",
    "            weights = self.parameters['W' + str(l)]\n",
    "            biases = self.parameters['b' + str(l)]\n",
    "            dZ = predicted_value - target_value\n",
    "            dW = np.dot(previous_activations, dZ.T)\n",
    "            db = np.sum(dZ)\n",
    "            self.parameters['W' + str(l)] -= learning_rate * dW\n",
    "            self.parameters['b' + str(l)] -= learning_rate * db\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=100, learning_rate=0.001):\n",
    "        for i in range(epochs):\n",
    "            Loss = []\n",
    "\n",
    "            for j in range(X_train.shape[0]):\n",
    "                X = X_train[j].reshape(2, 1)\n",
    "                y = y_train[j][0]\n",
    "\n",
    "                y_hat, A1 = self.L_layer_forward(X)\n",
    "                y_hat = y_hat[0][0]\n",
    "\n",
    "                self.update_parameters(y, y_hat, A1, X, learning_rate)\n",
    "\n",
    "                loss = -y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)\n",
    "                Loss.append(loss)\n",
    "\n",
    "            print('Epoch - ', i + 1, 'Loss - ', np.array(Loss).mean())\n",
    "\n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b05fd0fa-f5fd-4557-88e7-bf3e679be283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  0.6942130084533507\n",
      "Epoch -  2 Loss -  0.6942018592062668\n",
      "Epoch -  3 Loss -  0.6941907703485948\n",
      "Epoch -  4 Loss -  0.6941797415098183\n",
      "Epoch -  5 Loss -  0.6941687723218626\n",
      "Epoch -  6 Loss -  0.6941578624190763\n",
      "Epoch -  7 Loss -  0.6941470114382163\n",
      "Epoch -  8 Loss -  0.6941362190184293\n",
      "Epoch -  9 Loss -  0.6941254848012365\n",
      "Epoch -  10 Loss -  0.6941148084305158\n",
      "Epoch -  11 Loss -  0.6941041895524864\n",
      "Epoch -  12 Loss -  0.6940936278156918\n",
      "Epoch -  13 Loss -  0.6940831228709841\n",
      "Epoch -  14 Loss -  0.6940726743715075\n",
      "Epoch -  15 Loss -  0.6940622819726829\n",
      "Epoch -  16 Loss -  0.6940519453321908\n",
      "Epoch -  17 Loss -  0.6940416641099572\n",
      "Epoch -  18 Loss -  0.6940314379681368\n",
      "Epoch -  19 Loss -  0.6940212665710981\n",
      "Epoch -  20 Loss -  0.6940111495854071\n",
      "Epoch -  21 Loss -  0.6940010866798134\n",
      "Epoch -  22 Loss -  0.6939910775252339\n",
      "Epoch -  23 Loss -  0.6939811217947378\n",
      "Epoch -  24 Loss -  0.6939712191635322\n",
      "Epoch -  25 Loss -  0.6939613693089467\n",
      "Epoch -  26 Loss -  0.6939515719104192\n",
      "Epoch -  27 Loss -  0.6939418266494808\n",
      "Epoch -  28 Loss -  0.6939321332097402\n",
      "Epoch -  29 Loss -  0.6939224912768723\n",
      "Epoch -  30 Loss -  0.6939129005386004\n",
      "Epoch -  31 Loss -  0.6939033606846843\n",
      "Epoch -  32 Loss -  0.6938938714069045\n",
      "Epoch -  33 Loss -  0.6938844323990501\n",
      "Epoch -  34 Loss -  0.6938750433569029\n",
      "Epoch -  35 Loss -  0.6938657039782248\n",
      "Epoch -  36 Loss -  0.6938564139627432\n",
      "Epoch -  37 Loss -  0.6938471730121389\n",
      "Epoch -  38 Loss -  0.6938379808300299\n",
      "Epoch -  39 Loss -  0.6938288371219609\n",
      "Epoch -  40 Loss -  0.6938197415953881\n",
      "Epoch -  41 Loss -  0.693810693959666\n",
      "Epoch -  42 Loss -  0.6938016939260354\n",
      "Epoch -  43 Loss -  0.693792741207609\n",
      "Epoch -  44 Loss -  0.6937838355193594\n",
      "Epoch -  45 Loss -  0.6937749765781054\n",
      "Epoch -  46 Loss -  0.6937661641025\n",
      "Epoch -  47 Loss -  0.6937573978130174\n",
      "Epoch -  48 Loss -  0.6937486774319399\n",
      "Epoch -  49 Loss -  0.6937400026833462\n",
      "Epoch -  50 Loss -  0.6937313732930988\n",
      "Epoch -  51 Loss -  0.6937227889888307\n",
      "Epoch -  52 Loss -  0.6937142494999347\n",
      "Epoch -  53 Loss -  0.6937057545575496\n",
      "Epoch -  54 Loss -  0.6936973038945502\n",
      "Epoch -  55 Loss -  0.6936888972455332\n",
      "Epoch -  56 Loss -  0.693680534346806\n",
      "Epoch -  57 Loss -  0.6936722149363758\n",
      "Epoch -  58 Loss -  0.6936639387539365\n",
      "Epoch -  59 Loss -  0.6936557055408584\n",
      "Epoch -  60 Loss -  0.6936475150401753\n",
      "Epoch -  61 Loss -  0.6936393669965739\n",
      "Epoch -  62 Loss -  0.6936312611563822\n",
      "Epoch -  63 Loss -  0.6936231972675583\n",
      "Epoch -  64 Loss -  0.6936151750796784\n",
      "Epoch -  65 Loss -  0.693607194343927\n",
      "Epoch -  66 Loss -  0.6935992548130849\n",
      "Epoch -  67 Loss -  0.6935913562415181\n",
      "Epoch -  68 Loss -  0.6935834983851676\n",
      "Epoch -  69 Loss -  0.6935756810015375\n",
      "Epoch -  70 Loss -  0.6935679038496859\n",
      "Epoch -  71 Loss -  0.6935601666902123\n",
      "Epoch -  72 Loss -  0.6935524692852479\n",
      "Epoch -  73 Loss -  0.6935448113984461\n",
      "Epoch -  74 Loss -  0.6935371927949702\n",
      "Epoch -  75 Loss -  0.6935296132414839\n",
      "Epoch -  76 Loss -  0.6935220725061408\n",
      "Epoch -  77 Loss -  0.6935145703585752\n",
      "Epoch -  78 Loss -  0.6935071065698903\n",
      "Epoch -  79 Loss -  0.6934996809126488\n",
      "Epoch -  80 Loss -  0.6934922931608636\n",
      "Epoch -  81 Loss -  0.6934849430899868\n",
      "Epoch -  82 Loss -  0.6934776304769006\n",
      "Epoch -  83 Loss -  0.6934703550999066\n",
      "Epoch -  84 Loss -  0.6934631167387174\n",
      "Epoch -  85 Loss -  0.6934559151744458\n",
      "Epoch -  86 Loss -  0.6934487501895957\n",
      "Epoch -  87 Loss -  0.6934416215680528\n",
      "Epoch -  88 Loss -  0.6934345290950741\n",
      "Epoch -  89 Loss -  0.6934274725572802\n",
      "Epoch -  90 Loss -  0.6934204517426447\n",
      "Epoch -  91 Loss -  0.6934134664404853\n",
      "Epoch -  92 Loss -  0.6934065164414542\n",
      "Epoch -  93 Loss -  0.6933996015375306\n",
      "Epoch -  94 Loss -  0.6933927215220096\n",
      "Epoch -  95 Loss -  0.6933858761894942\n",
      "Epoch -  96 Loss -  0.6933790653358868\n",
      "Epoch -  97 Loss -  0.693372288758379\n",
      "Epoch -  98 Loss -  0.6933655462554447\n",
      "Epoch -  99 Loss -  0.6933588376268295\n",
      "Epoch -  100 Loss -  0.693352162673543\n",
      "Trained Parameters:\n",
      "W1 : [[0.09313675 0.09313675]\n",
      " [0.09313675 0.09313675]]\n",
      "b1 : [[-0.01471859]\n",
      " [-0.01471859]]\n",
      "W2 : [[0.09313675]\n",
      " [0.09313675]]\n",
      "b2 : [[-0.01471859]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the NeuralNetworkClassification class\n",
    "layer_dims = [2, 2, 1]\n",
    "nn_classification = NeuralNetworkClassification(layer_dims)\n",
    "\n",
    "# Define the input features (X_train) and target variable (y_train)\n",
    "X_train = np.array([[8, 8], [7, 9], [6, 10], [5, 5]])\n",
    "y_train = np.array([[1], [1], [0], [0]])\n",
    "\n",
    "# Train the model\n",
    "trained_parameters = nn_classification.train(X_train, y_train)\n",
    "\n",
    "# Print the trained parameters\n",
    "print(\"Trained Parameters:\")\n",
    "for key, value in trained_parameters.items():\n",
    "    print(key, \":\", value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
