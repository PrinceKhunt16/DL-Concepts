{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749f5534-f631-4763-89dc-693f7f348676",
   "metadata": {},
   "source": [
    "**`__init__` method**: The constructor initializes the neural network model by calling the `initialize_parameters` method to initialize the weights and biases.\n",
    "\n",
    "**`initialize_parameters` method**: This method initializes the parameters of the neural network (weights and biases) with random values. It takes a list `layer_dims` as input, which specifies the number of nodes in each layer of the network.\n",
    "\n",
    "**`linear_forward` method**: This method performs the linear transformation step of a single layer in the neural network. It takes the input activations `A_prev`, weight matrix `W`, and bias vector `b` as inputs, and computes the linear combination `Z = W.T * A_prev + b`.\n",
    "\n",
    "**`L_layer_forward` method**: This method implements the forward propagation step of the entire neural network. It iterates through each layer of the network, applies the linear transformation followed by the activation function (in this case, the sigmoid function), and computes the output activations `A`. It returns the final output activations `A` and the activations of the previous layer `A_prev`.\n",
    "\n",
    "**`update_parameters` method**: This method updates the parameters (weights and biases) of the neural network using the gradient descent algorithm. It takes the true target value `y`, predicted value `y_hat`, activations of the first layer `A1`, input data `X`, and learning rate `learning_rate` as inputs. It computes the gradients of the loss function with respect to the parameters and updates the parameters accordingly.\n",
    "\n",
    "**`train` method**: This method trains the neural network using the provided training data (`X_train`, `y_train`). It iterates through multiple epochs, where each epoch consists of iterating through each training example, computing the predicted output, updating the parameters, and calculating the loss. It prints the loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff2042a-cd82-44a9-ba68-9b9f935cee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetworkRegression:\n",
    "    def __init__(self, layer_dimensions):\n",
    "        self.parameters = self.initialize_parameters(layer_dimensions)\n",
    "    \n",
    "    def initialize_parameters(self, layer_dimensions):\n",
    "        np.random.seed(3)\n",
    "        parameters = {}\n",
    "        num_layers = len(layer_dimensions)\n",
    "\n",
    "        for l in range(1, num_layers):\n",
    "            input_dim = layer_dimensions[l-1]\n",
    "            output_dim = layer_dimensions[l]\n",
    "            parameters['W' + str(l)] = np.ones((input_dim, output_dim)) * 0.1\n",
    "            parameters['b' + str(l)] = np.zeros((output_dim, 1))\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def linear_forward(self, previous_activations, weights, biases):\n",
    "        linear_output = np.dot(weights.T, previous_activations) + biases\n",
    "        return linear_output\n",
    "\n",
    "    def L_layer_forward(self, input_data):\n",
    "        activations = input_data\n",
    "        num_layers = len(self.parameters) // 2 \n",
    "\n",
    "        for l in range(1, num_layers + 1):\n",
    "            previous_activations = activations\n",
    "            weights = self.parameters['W' + str(l)]\n",
    "            biases = self.parameters['b' + str(l)]\n",
    "            activations = self.linear_forward(previous_activations, weights, biases)\n",
    "            \n",
    "        return activations, previous_activations\n",
    "\n",
    "    def update_parameters(self, target_value, predicted_value, previous_activations, input_data, learning_rate=0.001):\n",
    "        # Update parameters for the output layer\n",
    "        self.parameters['W2'][0][0] += learning_rate * 2 * (target_value - predicted_value) * previous_activations[0][0]\n",
    "        self.parameters['W2'][1][0] += learning_rate * 2 * (target_value - predicted_value) * previous_activations[1][0]\n",
    "        self.parameters['b2'][0][0] += learning_rate * 2 * (target_value - predicted_value)\n",
    "\n",
    "        # Update parameters for the hidden layer\n",
    "        self.parameters['W1'][0][0] += learning_rate * 2 * (target_value - predicted_value) * self.parameters['W2'][0][0] * input_data[0][0]\n",
    "        self.parameters['W1'][0][1] += learning_rate * 2 * (target_value - predicted_value) * self.parameters['W2'][0][0] * input_data[1][0]\n",
    "        self.parameters['b1'][0][0] += learning_rate * 2 * (target_value - predicted_value) * self.parameters['W2'][0][0]\n",
    "\n",
    "        self.parameters['W1'][1][0] += learning_rate * 2 * (target_value - predicted_value) * self.parameters['W2'][1][0] * input_data[0][0]\n",
    "        self.parameters['W1'][1][1] += learning_rate * 2 * (target_value - predicted_value) * self.parameters['W2'][1][0] * input_data[1][0]\n",
    "        self.parameters['b1'][1][0] += learning_rate * 2 * (target_value - predicted_value) * self.parameters['W2'][1][0]\n",
    "\n",
    "    def train(self, training_data, target_values, epochs=5, learning_rate=0.001):\n",
    "        for i in range(epochs):\n",
    "            losses = []\n",
    "\n",
    "            for j in range(training_data.shape[0]):\n",
    "                input_data = training_data[j].reshape(2, 1)\n",
    "                target_value = target_values[j][0]\n",
    "\n",
    "                predicted_value, previous_activations = self.L_layer_forward(input_data)\n",
    "                predicted_value = predicted_value[0][0]\n",
    "\n",
    "                self.update_parameters(target_value, predicted_value, previous_activations, input_data, learning_rate)\n",
    "\n",
    "                losses.append((target_value - predicted_value) ** 2)\n",
    "\n",
    "            print('Epoch - ', i + 1, 'Loss - ', np.array(losses).mean())\n",
    "\n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf673884-289e-4add-9451-a32f11d92e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  26.28249792398698\n",
      "Epoch -  2 Loss -  19.438253848220803\n",
      "Epoch -  3 Loss -  10.13987443582752\n",
      "Epoch -  4 Loss -  3.385561305106485\n",
      "Epoch -  5 Loss -  1.3198454128484565\n",
      "Trained Parameters:\n",
      "W1 : [[0.273603   0.3993222 ]\n",
      " [0.28787155 0.42586102]]\n",
      "b1 : [[0.02885522]\n",
      " [0.03133223]]\n",
      "W2 : [[0.42574893]\n",
      " [0.50219328]]\n",
      "b2 : [[0.11841278]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the NeuralNetworkRegression class\n",
    "layer_dims = [2, 2, 1]\n",
    "nn_regression = NeuralNetworkRegression(layer_dims)\n",
    "\n",
    "# Define the input features (X_train) and target variable (y_train)\n",
    "X_train = np.array([[8, 8], [7, 9], [6, 10], [5, 12]])\n",
    "y_train = np.array([[4], [5], [6], [7]])\n",
    "\n",
    "# Train the model\n",
    "trained_parameters = nn_regression.train(X_train, y_train)\n",
    "\n",
    "# Print the trained parameters\n",
    "print(\"Trained Parameters:\")\n",
    "for key, value in trained_parameters.items():\n",
    "    print(key, \":\", value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
